{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "This lesson is going to walkthrough the diamonds dataset and illustrate some best practices for Exploratory Data Analysis (EDA). This notebook is based on the EDA chapter in the [R for Data Science](https://r4ds.had.co.nz/exploratory-data-analysis.html#questions) book by Hadley Wickham. I refer you to that chapter for full details - since that book only has examples in the R language, we've Pythonised them here for you.\n",
    "\n",
    "## What is Exploratory Data Analysis?\n",
    "Quote dumping from the chapter: \n",
    "\n",
    "```\n",
    "(EDA is the process of using)... visualisation and transformation to explore your data in a systematic way ... EDA is an iterative cycle. You want to:\n",
    "\n",
    "1. Generate questions about your data.\n",
    "\n",
    "2. Search for answers by visualising, transforming, and modelling your data.\n",
    "\n",
    "3, Use what you learn to refine your questions and/or generate new questions.\n",
    "\n",
    "EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others.\n",
    "\n",
    "EDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. To do data cleaning, you’ll need to deploy all the tools of EDA: visualisation, transformation, and modelling.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous vs Categorical Variables\n",
    "\n",
    "A variable is **categorical** if it can only take one of a small set of values/ be members of a group. \n",
    "A categorical variable is **nominal** if there is no rank order between the set of values. E.g. Colours like Red, Blue, Green or Sex like Male and Female.\n",
    "A categorical variance is **ordinal** if there exists a rank order between the set of values/ groups. E.g. Finishing places in a race like 1st, 2nd, 3rd or rating scale responses like Very Bad, Bad, OK, Good, Very Good.\n",
    "\n",
    "A variable is **continuous** if it can take any of an infinite set of ordered values. E.g. Numerical values such as temperature or price. \n",
    "Note: this doesn't mean there aren't restrictions on the values. For instance, a price must be positive but among the positive numbers there is an infinite range it can take. There is an infinity of values between any 2 numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Questions\n",
    "\n",
    "The goal of EDA is to develop an understanding of the data. We do this by generating a large quantity of questions to ask about our data. As we answer our questions, new questions will arise which will be more particular and relevant to our dataset. As Hadley Wickham says:\n",
    "\n",
    "```\n",
    "There is no rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as:\n",
    "\n",
    "What type of variation occurs within my variables?\n",
    "\n",
    "What type of covariation occurs between my variables?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation\n",
    "Variation is the tendency of the values of a variable to change from measurement to measurement.\n",
    "Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable’s values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Diamonds Data\n",
    "diamonds = pd.read_csv(\"../data/diamonds.csv\")\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Diamonds Dataset\n",
    "This dataset contains information about 53,940 round-cut diamonds. How do we know? Each row of data represents a different diamond and there are 53,940 rows of data.\n",
    "\n",
    "There are 10 variables measuring various pieces of information about the diamonds.\n",
    "\n",
    "There are 3 categorical variables: cut, color, & clarity. For example, there are 5 categories of diamond cuts with “Fair” being the lowest grade of cut to ideal being the highest grade.\n",
    "\n",
    "There are 6 variables that are of numeric/ continuous structure: carat, depth, table, x, y, z\n",
    "\n",
    "There is 1 variable that has an integer/ categorical structure: price\n",
    "\n",
    "1. price:\tprice in US dollars\t`$326-$18,823`\n",
    "2. carat:\tweight of the diamond\t`0.2-5.01`\n",
    "3. cut:\tquality of the cut\t`Fair, Good, Very Good, Premium, Ideal`\n",
    "4. color:\tdiamond color\t`J (worst) to D (best)`\n",
    "5. clarity:\tmeasurement of how clear the diamond is\t`I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)`\n",
    "6. x:\tlength in mm\t`0-10.74`\n",
    "7. y:\twidth in mm\t`0-58.9`\n",
    "8. z:\tdepth in mm\t`0-31.8`\n",
    "9. depth:\ttotal depth percentage\t`43-79`\n",
    "10. table:\twidth of top of diamond relative to widest point\t`43-95`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute how many diamonds we have for each cut quality. For this, we can use the size method.\n",
    "diamonds_count = diamonds.groupby(by = ['cut'], as_index=False).size()\n",
    "diamonds_count = diamonds_count.rename(columns = {'size':'count'})\n",
    "diamonds_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical variables, barplots can be a good way to get a sense of the data\n",
    "sns.barplot(x = 'cut', y = 'count', data= diamonds_count, order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a historgram for carat types for diamonds\n",
    "sns.displot(diamonds['carat'], binwidth = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our plot looks streched because there are some diamonds with very high carats (outliers)\n",
    "diamonds['carat'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select diamonds with carat less than 3 (most of the data falls within that) and view the distribution\n",
    "smaller = diamonds[diamonds.carat < 3]\n",
    "\n",
    "sns.displot(smaller['carat'], binwidth = 0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at carat Histogram by diamond cut quality\n",
    "for key, value in enumerate(['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']):\n",
    "    smaller_cut = smaller[smaller.cut == value]\n",
    "    sns.displot(smaller_cut['carat'], binwidth = 0.1)\n",
    "    plt.title(value + ' Cut Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Values\n",
    "\n",
    "General Questions\n",
    "1. Which values are the most common? Why?\n",
    "\n",
    "2. Which values are rare? Why? Does that match your expectations?\n",
    "\n",
    "3. Can you see any unusual patterns? What might explain them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the histogram below for example\n",
    "sns.displot(smaller['carat'], binwidth = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask questions like:\n",
    "\n",
    "1. Why are there more diamonds at whole carats and common fractions of carats?\n",
    "\n",
    "2. Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak?\n",
    "\n",
    "3. Why are there no diamonds bigger than 3 carats?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at Clusters Within your Data\n",
    "\n",
    "Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask:\n",
    "\n",
    "1. How are the observations within each cluster similar to each other?\n",
    "\n",
    "2. How are the observations in separate clusters different from each other?\n",
    "\n",
    "3. How can you explain or describe the clusters?\n",
    "\n",
    "4. Why might the appearance of clusters be misleading?\n",
    "\n",
    "The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "faithful = pd.read_csv(\"../data/faithful.csv\")\n",
    "faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(faithful['eruptions'], binwidth = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Values/ Outliers\n",
    "\n",
    "Outliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers are hard to see with a histogram - usually you can tell with the limits on the x-axis. The y variable is the width in mm for the diamond.\n",
    "sns.displot(diamonds['y'], binwidth = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better way is to look at the percentiles for the distribution\n",
    "diamonds['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively, we can ask for custom percentiles\n",
    "np.percentile(diamonds['y'], [0, 5, 25, 50, 75, 90, 95, 99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that there are unusual values like 0 and 58. We can look at this data more closely\n",
    "filt_diamonds = diamonds[(diamonds.y < 3) | (diamonds.y > 20)].reset_index(drop = True)\n",
    "filt_diamonds[['price', 'x', 'y', 'z']].sort_values(by = ['y'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the y variable measures one of the 3 spatial dimensions of the diamonds, in mm, and we know diamonds can't have a width of 0mm. So these values must be incorrect. The measurements of 32mm and 59mm must also be implausible because they're massive diamonds but don't cost hundreds of thousands of dollars.\n",
    "\n",
    "More advice from Hadley Wickham:\n",
    "```\n",
    "It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "There are 2 options to dealing with unusual/ missing values in your dataset.\n",
    "1. Remove those values and continue \n",
    "2. Replace unusual values with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds['new_y'] = np.select([diamonds.y < 3, diamonds.y > 20], [np.NaN, np.NaN], default =  diamonds.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with unusual values\n",
    "sns.scatterplot(x = 'x', y = 'y', data = diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set unusual values as missing and then plot \n",
    "sns.scatterplot(x = 'x', y = 'new_y', data = diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other times you want to understand what makes observations with missing values **different** to observations with recorded values. \n",
    "\n",
    "We're now going to look at the New York City flights data from 2013, it includes on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013.\n",
    "\n",
    "For example, in the flights data, missing values in the `dep_time` (in the format HMM) variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and non-cancelled times. You can do this by making a new variable by checking if it's `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "flights = pd.read_csv(\"../data/nyc_flights.csv\")\n",
    "\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to indicate when a flight is cancelled\n",
    "flights['cancelled'] = np.where(flights.dep_time.isna(), True, False)\n",
    "flights['sched_hour'] = flights.sched_dep_time // 100\n",
    "flights['sched_min'] = flights.sched_dep_time % 100\n",
    "flights['sched_dep_time'] = (flights.sched_hour + flights.sched_min)/ 60\n",
    "\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_grped = flights.groupby(by = [\"sched_dep_time\", \"cancelled\"], as_index = False).size()\n",
    "flights_grped.columns = ['sched_dep_time', 'cancelled', 'count']\n",
    "flights_grped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's hard to compare these lines because there are so many non-cancelled flights vs cancelled\n",
    "sns.lineplot(x = \"sched_dep_time\", y = \"count\", hue = \"cancelled\", data = flights_grped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariation\n",
    "\n",
    "If variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. \n",
    "\n",
    "Since we have categorical and continuous variable types, measuring covariation between them will require different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical and Continuous Variables\n",
    "We can use boxplots to visualise how distributions vary between categories. Looking at these can be very instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = \"cut\", y = \"price\", data = diamonds, order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boxplots show (surprisingly) that better quality diamonds are cheaper on average. Maybe you can look into this data and find out why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Categorical Variables\n",
    "We can count frequencies between 2 categorical variables and visualise their relationships with heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A contingency table can be helpful here\n",
    "diamonds_contingency_table = pd.crosstab(diamonds['cut'], diamonds['color'])\n",
    "diamonds_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_counts = diamonds.groupby(by = ['color', 'cut'], as_index=False).size()\n",
    "diamonds_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(diamonds_contingency_table, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import association\n",
    "\n",
    "print(\"cramers v: \" + str(association(diamonds_contingency_table, method = 'cramer')) )\n",
    "print(\"tschuprow: \" + str(association(diamonds_contingency_table, method = 'tschuprow')) )\n",
    "print(\"pearson: \" + str(association(diamonds_contingency_table, method = 'pearson')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Continuous Variables\n",
    "The best way to measure the relationship and associations between continuous variables is to use scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = \"carat\", y = \"price\", data = diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a correlation matrix\n",
    "diamonds[['carat', 'price']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patterns and Models\n",
    "\n",
    "This is the first step towards machine learning. From Hadley Wickham\n",
    "\n",
    "```\n",
    "Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:\n",
    "\n",
    "Could this pattern be due to coincidence (i.e. random chance)?\n",
    "\n",
    "How can you describe the relationship implied by the pattern?\n",
    "\n",
    "How strong is the relationship implied by the pattern?\n",
    "\n",
    "What other variables might affect the relationship?\n",
    "\n",
    "Does the relationship change if you look at individual subgroups of the data?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patterns reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.\n",
    "\n",
    "Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to **remove** the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how price differs by cut\n",
    "sns.boxplot(x = 'cut', y = 'price', data = diamonds, order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a linear regression model\n",
    "diamonds_train = diamonds[['carat', 'price', 'cut']].copy()\n",
    "diamonds_train['log_carat'] = np.log(diamonds['carat'])\n",
    "diamonds_train['log_price'] = np.log(diamonds['price'])\n",
    "\n",
    "diamonds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(np.array(diamonds_train.log_carat).reshape(-1, 1), diamonds_train.log_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute exponential residuals. These are model errors, the difference between actual values and model predictions. We're removing the impact of carat in price and then we see how price differs by cut.\n",
    "exp_residuals = np.exp(diamonds_train.log_price.values - reg.predict(np.array(diamonds_train.log_carat).reshape(-1, 1)) )\n",
    "\n",
    "diamonds_train['resid'] = exp_residuals\n",
    "diamonds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.\n",
    "sns.boxplot(x = 'cut', y = 'resid', data = diamonds_train, order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Checklist\n",
    "These are some questions to ask yourself as you approach a new dataset. This list isn't exhaustive but can be a useful guide for you to ask your own questions.\n",
    "\n",
    "1. How is my data distributed (Continuous Variables)? This is best discovered by looking at the quantiles of the distributions returned by the `describe` method.\n",
    "\n",
    "2. What are the most frequent/ typical values in my dataset? Are these what I would expect to see?\n",
    "\n",
    "3. What are the extreme values/ outliers in my data? What could be the reason these values are present? How many of them are there and does this change by variable? Does my analysis change much with or without these values?\n",
    "\n",
    "4. Are there any missing values or incorrect values? What could be causing them?\n",
    "\n",
    "5. Are there any natural groupings in my data? If so, how does my data differ between these groupings?\n",
    "\n",
    "6. Of the key variables of interest in my data, what is the strength of associations between them?\n",
    "\n",
    "7. Is my data properly scaled so I can compare like with like?\n",
    "\n",
    "8. Could key variables in my dataset be systematically related? Could I model this relationship?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "The Art of Statistics by David Speigelhalter"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96cff63d46835ee138d10fd39d21d7d26297495ac413b2cb20db842db91a06fd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('muslamic_makers_intro_data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
